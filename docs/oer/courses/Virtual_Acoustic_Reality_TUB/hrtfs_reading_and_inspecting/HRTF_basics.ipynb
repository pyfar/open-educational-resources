{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb2c482c8440524fdc3e27ac8fb27d58",
     "grade": false,
     "grade_id": "Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# HRTFs: Reading and Inspecting\n",
    "*Fabian Brinkmann, Anton Hoyer*<br>  \n",
    "*Audio Communication Group, Technische Universität Berlin*<br>  \n",
    "*Contact: fabian.brinkmann@tu-berlin.de*\n",
    "\n",
    "Head-related transfer functions (HRTFs) are fundamental to 3D audio. In this notebook, you will learn how to read and visualize HRTFs, and how to estimate (broadband) interaural time and level differences from HRTFs.\n",
    "\n",
    "**Duration:** 45-60 Minutes\n",
    "\n",
    "**Requirements:** Basic knowledge of spatial hearing, HRTFs, coordinate conventions, and SOFA files\n",
    "\n",
    "**References**<br>  \n",
    "[1] F. Brinkmann and C. Pike, “Binauraltechnik,” in Handbuch der Audiotechnik, 2nd ed., S. Weinzierl, Ed., Berlin, Heidelberg: Springer, 2023. doi: [10.1007/978-3-662-60357-4_27-2](https://doi.org/10.1007/978-3-662-60357-4_27-2).<br>  \n",
    "[2] A. Andreopoulou and B. F. G. Katz, “Identification of perceptually relevant methods of inter-aural time difference estimation,” J. Acoust. Soc. Am., vol. 142, no. 2, pp. 588–598, Aug. 2017, doi: [10.1121/1.4996457](https://doi.org/10.1121/1.4996457)\n",
    "\n",
    "**Dependencies**<br>  \n",
    "`pip install pyfar>=0.7 pooch nbgrader ipykernel watermark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fe50e95bf7c67e3e43d328a90a371c8",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pyfar as pf\n",
    "import sofar as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pooch\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eff5075163f87e7d48f6eaea6312f45c",
     "grade": false,
     "grade_id": "Task1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Reading HRTFs\n",
    "\n",
    "HRTFs are stored in so called sofa-files. SOFA is a standardized file format for storing spatially distributed acoustic data, e.g., HRTFs. SOFA files contain the acoustic data along with meta data. In this case the most important meta data are the source positions at which the HRTFs are available.\n",
    "\n",
    "If you want to know more about the SOFA file standard, we recommend the [sofar documentation](https://sofar.readthedocs.io/en/stable/readme.html)\n",
    "\n",
    "Start by downloading the file `FABIAN_HRIR_measured_HATO_0.sofa` by executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this path to your needs. Using `None` will download the file to your\n",
    "# system cash.\n",
    "path = None\n",
    "\n",
    "# Leave this as it is: This is the URL from which the data will be downloaded\n",
    "# and a hash for checking if the download worked.\n",
    "url = 'https://github.com/pyfar/files/raw/refs/heads/main/education/VAR_TUB/FABIAN_HRIR_measured_HATO_0.sofa?download='\n",
    "hash = '83ebbcd9a09d17679b95d201c9775438c0bb1199d565c3fc7a25448a905cdc3c'\n",
    "\n",
    "file = pooch.retrieve(\n",
    "    url, hash, fname='FABIAN_HRIR_measured_HATO_0.sofa', path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the file. Your goal is to get the HRIRs as a pyfar Signal and the source positions at which the HRIRs were measured as a pyfar Coordinates object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8032c9bbc628f900342af45382ed28d0",
     "grade": true,
     "grade_id": "Solution1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# %% read HRTFs from SOFA file ------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85f9499018a05ac687127e9f41515592",
     "grade": false,
     "grade_id": "Task2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Source Positions\n",
    "\n",
    "A plethora of HRTF datasets are freely available online and they were all measured for different sampling grids, i.e., different source positions. A good first step is to get an idea of the sampling grid to know how many HRTFs were measured and how they are distributed in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a07a2b94585c9955d8aafb8812fc1703",
     "grade": true,
     "grade_id": "Solution2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# %% print the number of source positions and plot their spatial position -----\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19f2b168b31a1da23310203ef2b7866d",
     "grade": false,
     "grade_id": "Task3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Visualization and feature estimation for a single HRIR\n",
    "\n",
    "Whenever you are dealing with third party data or measuring your own data, it is a good idea to get familiar with it, e.g., by visualization.\n",
    "\n",
    "1. Find and plot HRIRS and HRTF for a single source position and check your result for plausibilty: Does your plot look as expected? Typical source positions for this inspection could be sources to the front/back, left/right, and above/below the subject. What angles describe these source positions, and how do you get the corresponding HRIRs from the Signal and Coordinates objects?\n",
    "2. Once you finished the plot, estimate the **broadband** interaural time and level difference (ITD and ILD) for the source positions and display it as part of the plot.\n",
    "\n",
    "- **ITD estimation:** There are multiple ways to estimate the broadband ITD (see [2]). A simple yet perceptually meaning full approach is to low-pass the HRIR at 3 kHz and then find the first sample in the HRIR that is less then 20 or 10 dB below the absolute maximum of the HRIR (per channel). To estimate the ITD with subsample precision, it can be resamples to a higher sampling rate before.\n",
    "- **ILD estimation:** A simple way to estimate the broadband ILD is to compute the energy of the left and right ear HRIR and compute their ratio in decibel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b5842e8774459257e9b9e8e02a262fe",
     "grade": true,
     "grade_id": "Solution3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# %% plot HRTF for single source position -------------------------------------\n",
    "# use the pyfar default plot style\n",
    "pf.plot.use()\n",
    "\n",
    "# define the desired source position\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# find the closest source position in data\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# estimate ITD ----------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# estimate ILD ------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# plot --------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fe153d031b7ae65f07519d465bc6c8f",
     "grade": false,
     "grade_id": "Note3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that the onsets were computed on the lowpassed HRIRs. Hence they appear earlier than one might expect in the plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1f16cfc7550e5b3c86fc91d16497ac0",
     "grade": false,
     "grade_id": "Task4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Plot multiple HRIRs\n",
    "\n",
    "Plot all HRIRs and HRTFs in the horizontal and median plane. You can do this by color coding the amplitude of the HRIRs and HRTFs using pyfars 2D plots.\n",
    "\n",
    "- How are the horizontal and median plane defined?\n",
    "- What is the easiest way to find the corresponding source positions?\n",
    "- What do you see in the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61b178a5e30d464f666f9cd5d2de4ecf",
     "grade": true,
     "grade_id": "Solution4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# %% plot horizontal and median plane -----------------------------------------\n",
    "for plane in ['Horizontal', 'Median']:\n",
    "    if plane == 'Horizontal':\n",
    "        # find and sort source positions and HRTFs in the horizontal plane\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        # find and sort source positions and HRTFs in the median plane\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # plot the selected HRTFs in the time and frequency domain\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c52f718f5b6801f76615b1990bb96023",
     "grade": false,
     "grade_id": "TaskOptional",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Optional: Feature estimation in auditory resolution\n",
    "\n",
    "You estimated the broadband interaural time and level differences (ITD and ILD) above. Sometimes a frequency-dependent analysis of these (or other) features is of interest. Estimate and visualize the frequency-dependent ILD for a single HRTF with auditory resolution using pyfar's Gammatone filter bank.\n",
    "\n",
    "You can also estimate the frequency dependent ITD if you want. In this case it would be more common to estimate the ITD by computing the cross-correlation between left and right ear signals and then find the time shift that yields the maximum cross-correlation. This can be done with `pyfar.dsp.correlate` introduced in pyfar 0.8.0.\n",
    "\n",
    "Depending on how important it is to model actual hearing, additional pre-processing such as half-wave rectification and square root compression is applied as well to model the behavior of the middle and inner ear with more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a58da6532fd63ad177a706c4068def55",
     "grade": true,
     "grade_id": "SolutionOptional",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# %% compute and plot frequency-dependent ILD ---------------------------------\n",
    "# select an HRIR\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# filter in auditory bands (zero pad HRIRs to increase FFT resolution)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# compute ILD in auditory bands\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Visualize\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License notice\n",
    "\n",
    "This notebook is licensed under CC BY 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watermark\n",
    "\n",
    "The following watermark might help others to install specific package versions that might be required to run the notebook. Please give at least the versions of Python, IPython, numpy , and scipy, major third party packagers (e.g., pytorch), and all used pyfar packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,scipy,pyfar,sofar,nbgrader,watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfar_oer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
